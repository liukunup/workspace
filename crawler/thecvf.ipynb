{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08717680-8435-4032-ac79-85964f1f45ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "# author : Liu Kun\n",
    "# date   : 2021-09-01 00:00:01\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def request_paper_list(url):\n",
    "    # 请求页面\n",
    "    base_url = f\"https://openaccess.thecvf.com\"\n",
    "    response = requests.get(url)\n",
    "    # 使用BeautifulSoup加载页面\n",
    "    soup = BeautifulSoup(response.content, features=\"lxml\")\n",
    "    # 按标签查找\n",
    "    dl = soup.find(\"dl\")\n",
    "    dt_list = dl.find_all(\"dt\")\n",
    "    dd_list = dl.find_all(\"dd\")\n",
    "    # 去除Back\n",
    "    if \"back\" in dd_list[0].text.strip().lower():\n",
    "        dd_list.pop(0)\n",
    "    # 按奇偶处理dd标签\n",
    "    dd_odd_list = dd_list[::2]\n",
    "    dd_even_list = dd_list[1::2]\n",
    "    # 用于存放论文信息\n",
    "    paper_list = list()\n",
    "    # 逐行处理\n",
    "    for item in zip(dt_list, dd_odd_list, dd_even_list):\n",
    "        # Line 1\n",
    "        line_1 = item[0]\n",
    "        tag_a_1 = line_1.find(\"a\")\n",
    "        # Line 2\n",
    "        line_2 = item[1]\n",
    "        # Line 3\n",
    "        line_3 = item[2]\n",
    "        tag_a_2 = line_3.find(\"a\")\n",
    "        tag_div = line_3.find(\"div\")\n",
    "        paper_json = {\n",
    "            \"web_url\": base_url + tag_a_1[\"href\"],\n",
    "            \"title\": tag_a_1.text.strip(),\n",
    "            \"author\": line_2.text.strip().replace(\"\\n\", \"\").replace(\"\\r\", \"\"),\n",
    "            \"pdf_url\": base_url + tag_a_2[\"href\"],\n",
    "            \"infos\": tag_div.text.strip().replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"[bibtex]\", \"\"),\n",
    "        }\n",
    "        # print(\"-\" * 100)\n",
    "        # print(json.dumps(paper_json, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "        paper_list.append(paper_json)\n",
    "    print(\"共爬取到论文信息%d条\" % len(paper_list))\n",
    "    return paper_list\n",
    "\n",
    "\n",
    "def log(papers, logfile):\n",
    "    with open(logfile, \"a+\") as f:\n",
    "        for p in papers:\n",
    "            f.write(json.dumps(p) + \"\\n\")\n",
    "    pass\n",
    "\n",
    "\n",
    "def downloader(url, folder):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    file = os.path.join(folder, filename)\n",
    "    if os.path.exists(file):\n",
    "        print(f\"skip {file} @ {url}\")\n",
    "        return file\n",
    "    print(f\"download {file} from {url}\")\n",
    "    r = requests.get(url) \n",
    "    with open(file, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return file\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 从json加载待爬取信息\n",
    "    menu_list = list()\n",
    "    with open(\"menu.json\", \"r\") as f:\n",
    "        menu_list = json.load(f)\n",
    "    # 爬取论文信息\n",
    "    for item in menu_list:\n",
    "        # 打印待爬取信息\n",
    "        print(item)\n",
    "        sub_path = item[\"path\"]\n",
    "        # 如文件夹不存在则创建\n",
    "        if not os.path.exists(sub_path):\n",
    "            os.makedirs(sub_path)\n",
    "        log_filename = item[\"logfile\"]\n",
    "        target_url_list = item[\"links\"]\n",
    "        for target_url in target_url_list:\n",
    "            # 爬取论文链接\n",
    "            papers = request_paper_list(target_url)\n",
    "            # 记录论文信息到日志文件\n",
    "            log_file = os.path.join(sub_path, log_filename)\n",
    "            log(papers, log_file)\n",
    "    # 按文件夹下载\n",
    "    for item in menu_list:\n",
    "        paper_path = item[\"path\"]\n",
    "        paper_jsonl = item[\"logfile\"]\n",
    "        with open(f\"{paper_path}/{paper_jsonl}\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for l in lines:\n",
    "                obj = json.loads(l)\n",
    "                downloader(obj[\"pdf_url\"], paper_path)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31d24b-75e0-4f84-a500-458720c40253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
